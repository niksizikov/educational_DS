# Цель задания

Разработать пайплайн обучения ML-модели.

# Что нужно сделать

1. Скачать архив с проектом airflow_hw, внутри него:
    - шаблон DAG’а (dags/hw_dag.py),
    - готовый код ML-модели (modules/pipeline.py),
    - шаблон скрипта для прогноза моделью (modules/predict.py),
    - данные для обучения и тестирования (data/train, data/test),
    - пустые папки под сохранение ML-модели и предсказаний.

2. Положить папку airflow_hw в домашнюю директорию (~/) и открыть её в **Pycharm**.

3. Запустить пайплайн с моделью локально и в Airflow, это обучит и сохранит объект с пайплайном лучшей модели в **pickle** формате:
    - локально: python3 modules/pipeline.py (из терминала Pycharm).
    - в Airflow: скопировать файл hw_dag.py в папку $AIRFLOW_HOME/dags.

     После этого в интерфейсе отобразится новый DAG:

![image](https://github.com/user-attachments/assets/52e6779b-e791-4b32-a712-99b2a2f5f5e4)

4. Написать код в файле modules/predict.py, который при вызове функции predict():
    - загружает обученную модель,
    - делает предсказания для всех объектов в папке data/test,
    - объединяет предсказания в один Dataframe и сохраняет их в csv-формате в папку data/predictions.

     Не забываем разбивать код на смысловые части в виде отдельных функций.

5. Проверить корректность кода, запустив его локально: python3modules/predict.py (из терминала Pycharm)

6. Встроить прогноз моделью в пайплайн, в котором будет 2 шага:
    - pipeline — здесь выполняется функция pipeline,
    - predict — здесь делается предикт для всех объектов и сохраняется в папку data/predictions. 

7. Запустить пайплайн в интерфейсе Airflow и получить предикты модели.

# Советы и рекомендации

1. Целевая структура проекта выглядит так:
    
    - ![image](https://github.com/user-attachments/assets/c86869a6-5e08-4ade-a10e-470d980e5c01)

3. Работа с файлами проекта будет зависеть от способа развертывания Airflow:
    - **локально**: Airflow живёт в домашней директории (~/airflow) вашей операционной системы, папка с проектом располагается рядом (~/airflow_hw). При запуске DAG результаты будут записываться сразу в папку проекта, дополнительных действий не потребуется; содержимое файла предсказания можно посмотреть так:
    
    - ![image](https://github.com/user-attachments/assets/310bbd27-aaac-4e3e-b37d-7ad2b37498c3)
    
    - Инструкцию по развертыванию Airflow вы можете скачать в разделе «Материалы и заметки». 

# Критерии оценивания

1. Структура проекта и логика файлов modules/predict и dags/hw_dag соответствуют заданию.

2. DAG с пайплайном модели из двух шагов (pipeline, predict) работает и сохраняет предсказания в файл (приложен скриншот с содержимым файла с предиктами модели).

3. Папка Airflow содержит только файлы самого Airflow. В папку dags скопированы файлы DAG’ов из папки с проектом. (Файлы проекта и папка Airflow существуют раздельно.)

# Как отправить работу на проверку

1. Код проекта (папка airflow_hw) выложить на Github в личный репозиторий.

2. Для возможности проверки сделать его публичным (public).

3. Ссылку на репозиторий прикрепить через форму ниже, туда же скриншот с содержимым файла с предиктами модели, как результат работы итогового DAG.


P. S. Интересные факты, которые вы могли заметить при работе с Airflow:

1. В файле modules/pipeline.py вы наверняка заметите, что отладочная информация о качестве ML-моделей теперь выводится не через print(), а с помощью модуля logging. В реальных задачах справочную информацию принято логировать таким образом. Благодаря этому, в логах задач в Airflow мы сможем читать всю нужную нам информацию (время, файл и так далее):
    - ![image](https://github.com/user-attachments/assets/ee4852b2-3927-4b42-af6b-b26001a6545d)

2. Также вы можете заметить в коде typing — это конструкции вида func(value: int) -> str:.
Это означает, что функция func принимает аргумент типа int, а возвращает значение типа string. Это помогает понимать тип данных при написании кода. При этом надо помнить, что Python обладает динамической типизацией, и такая аннотация — это лишь подсказка для удобства разработчика.
3. В папке проекта есть файл .gitignore. Он поможет вам не отслеживать файлы с данными при выкладывании кода в ваш репозиторий GitHub:
    - ![image](https://github.com/user-attachments/assets/eff036d3-35db-4d22-9cd7-45c70aa86961)

# Материалы и заметки

### Инструкция по развертыванию Airflow:

Находится в модуле 33.2

## Примечания куратора

Локально запускать даг, как это делал лектор, не требуется – это сработает только для пользователей Unix-based систем, а в рамках задачи не несёт никакой полезной работы – проверять работу дага мы в любом случае будем через веб-интерфейс. Соответственно, работать с окружениями вам тоже не требуется, как и устанавливать Airflow локально – вы устанавливали его через Docker, что верно. 

Если скрипт предикта уже готов и отлажен, вам нужно положить модуль в контейнер worker, чтобы скрипты из модуля (pipeline и predict) были видны и запускались. Нигде в уроке это не было указано, но положить модуль нужно ещё и в контейнер scheduler, иначе веб-интерфейс будет показывать ошибку при импорте дага.

1. Узнаём id контейнера с воркером:

        docker ps | grep worker

2. Копируем исполняемый код и данные на воркер:

        docker cp ~/airflow_hw <worker_id>:/home/airflow/airflow_hw

Напомню, что "~" – это обозначение для корня пользовательской папки, т.е. C:\Users\<username>. После команды docker cp сначала указываем, что копируем, а затем то, куда копируем. Путь, куда копируем, трогать не нужно.

3. Заходим на воркер и ставим нужные для работы пайплайна пакеты:

    -     docker exec -it <worker_id> bash

    -     pip install scikit-learn

Если работаете из-под Windows, то в GitBash команде docker exec ещё нужно дописать winpty:

    winpty docker exec -it <worker_id> bash

**По аналогии делаем с scheduler-ом.**

P.S. Перед копированием скриптов на worker и scheduler, очистите, пожалуйста, папку с моделями (data/models). Это на случай, если на локалке стояли другие версии библиотек.

P.P.S. Валидировать, появились ли файлы в контейнере, можно зайдя в контейнер от root-пользователя:

    docker exec -it -u root id_конейтенра bash

И проверив содержимое директории, куда мы копировали скрипты:

    ls /home/airflow/airflow_hw
